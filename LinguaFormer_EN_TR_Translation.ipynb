{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T21:06:00.888881Z",
     "iopub.status.busy": "2025-05-10T21:06:00.888654Z",
     "iopub.status.idle": "2025-05-10T21:06:17.141746Z",
     "shell.execute_reply": "2025-05-10T21:06:17.141006Z",
     "shell.execute_reply.started": "2025-05-10T21:06:00.888864Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.1/792.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade rouge-score\n",
    "!pip install -q --upgrade keras-hub\n",
    "!pip install -q --upgrade keras # Upgrade to Keras 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-10T21:06:17.143336Z",
     "iopub.status.busy": "2025-05-10T21:06:17.143112Z",
     "iopub.status.idle": "2025-05-10T21:06:33.507669Z",
     "shell.execute_reply": "2025-05-10T21:06:33.506850Z",
     "shell.execute_reply.started": "2025-05-10T21:06:17.143315Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 21:06:19.364151: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746911179.663794      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746911179.741120      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import keras_hub\n",
    "import pathlib\n",
    "import random\n",
    "import keras\n",
    "from keras import ops\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tf_data\n",
    "from tensorflow_text.tools.wordpiece_vocab import (\n",
    "    bert_vocab_from_dataset as bert_vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T21:06:36.827254Z",
     "iopub.status.busy": "2025-05-10T21:06:36.826550Z",
     "iopub.status.idle": "2025-05-10T21:06:36.831081Z",
     "shell.execute_reply": "2025-05-10T21:06:36.830185Z",
     "shell.execute_reply.started": "2025-05-10T21:06:36.827229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 25\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "ENG_VOCAB_SIZE = 20000\n",
    "TR_VOCAB_SIZE = 20000\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T21:06:38.773098Z",
     "iopub.status.busy": "2025-05-10T21:06:38.772794Z",
     "iopub.status.idle": "2025-05-10T21:06:40.993681Z",
     "shell.execute_reply": "2025-05-10T21:06:40.992872Z",
     "shell.execute_reply.started": "2025-05-10T21:06:38.773074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "file_path = \"/kaggle/input/veriseti/Sentence pairs in English-Turkish - 2025-05-06.tsv\"\n",
    "try:\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) < 4:\n",
    "                continue\n",
    "            _, eng, _, tr = parts[:4]\n",
    "            text_pairs.append((eng.lower(), tr.lower()))\n",
    "except FileNotFoundError:\n",
    "    print(f\"HATA: Veri dosyası bulunamadı: {file_path}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T21:06:47.436304Z",
     "iopub.status.busy": "2025-05-10T21:06:47.435594Z",
     "iopub.status.idle": "2025-05-10T21:06:47.808623Z",
     "shell.execute_reply": "2025-05-10T21:06:47.807989Z",
     "shell.execute_reply.started": "2025-05-10T21:06:47.436277Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 712266 sentence pairs\n",
      "Example: ('those people contributed greatly to world peace.', 'bu insanlar dünya barışına çok büyük katkı sağladı.')\n",
      "712266 total pairs, 498586 training, 106839 validation, 106841 test pairs\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(text_pairs)} sentence pairs\")\n",
    "if not text_pairs:\n",
    "    print(\"Hiç cümle çifti yüklenemedi.\")\n",
    "    exit()\n",
    "print(\"Example:\", text_pairs[0])\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = int(0.70 * len(text_pairs))\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "print(f\"{len(text_pairs)} total pairs, {len(train_pairs)} training, {len(val_pairs)} validation, {len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T21:07:04.770946Z",
     "iopub.status.busy": "2025-05-10T21:07:04.770368Z",
     "iopub.status.idle": "2025-05-10T21:10:58.059473Z",
     "shell.execute_reply": "2025-05-10T21:10:58.058745Z",
     "shell.execute_reply.started": "2025-05-10T21:07:04.770924Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training English tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746911227.096538      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1746911227.097318      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Turkish tokenizer...\n",
      "English vocabulary size: 6299, Turkish vocabulary size: 11091\n"
     ]
    }
   ],
   "source": [
    "def train_word_piece(text_samples, vocab_size, reserved_tokens, max_samples=200_000, sample_frac=0.5):\n",
    "    n = len(text_samples)\n",
    "    target = min(max_samples, int(sample_frac * n))\n",
    "    text_samples_subset = random.sample(text_samples, target) if target < n else text_samples\n",
    "    ds = tf_data.Dataset.from_tensor_slices(text_samples_subset)\n",
    "    ds = ds.shuffle(buffer_size=len(text_samples_subset)).batch(2048).cache().prefetch(tf_data.AUTOTUNE)\n",
    "    return keras_hub.tokenizers.compute_word_piece_vocabulary(ds, vocabulary_size=vocab_size, reserved_tokens=reserved_tokens)\n",
    "\n",
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "print(\"Training English tokenizer...\")\n",
    "eng_vocab = train_word_piece([p[0] for p in train_pairs], ENG_VOCAB_SIZE, reserved_tokens)\n",
    "print(\"Training Turkish tokenizer...\")\n",
    "tr_vocab = train_word_piece([p[1] for p in train_pairs], TR_VOCAB_SIZE, reserved_tokens)\n",
    "print(f\"English vocabulary size: {len(eng_vocab)}, Turkish vocabulary size: {len(tr_vocab)}\")\n",
    "eng_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(vocabulary=eng_vocab, lowercase=False)\n",
    "tr_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(vocabulary=tr_vocab, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T21:11:07.715472Z",
     "iopub.status.busy": "2025-05-10T21:11:07.715206Z",
     "iopub.status.idle": "2025-05-10T21:11:17.939749Z",
     "shell.execute_reply": "2025-05-10T21:11:17.939169Z",
     "shell.execute_reply.started": "2025-05-10T21:11:07.715446Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset check:\n",
      "inputs[\"encoder_inputs\"].shape: (128, 25), inputs[\"decoder_inputs\"].shape: (128, 25), targets.shape: (128, 25)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_batch(eng, tr):\n",
    "    eng = eng_tokenizer(eng)\n",
    "    tr = tr_tokenizer(tr)\n",
    "    eng_packer = keras_hub.layers.StartEndPacker(sequence_length=MAX_SEQUENCE_LENGTH, pad_value=eng_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    tr_packer = keras_hub.layers.StartEndPacker(sequence_length=MAX_SEQUENCE_LENGTH + 1, start_value=tr_tokenizer.token_to_id(\"[START]\"), end_value=tr_tokenizer.token_to_id(\"[END]\"), pad_value=tr_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    return {\"encoder_inputs\": eng_packer(eng), \"decoder_inputs\": tr_packer(tr)[:, :-1]}, tr_packer(tr)[:, 1:]\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, tr_texts = zip(*pairs)\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((list(eng_texts), list(tr_texts)))\n",
    "    return dataset.batch(BATCH_SIZE).map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE).shuffle(2048).prefetch(tf_data.AUTOTUNE).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "print(\"\\nDataset check:\")\n",
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}, inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}, targets.shape: {targets.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T21:11:17.941504Z",
     "iopub.status.busy": "2025-05-10T21:11:17.941277Z",
     "iopub.status.idle": "2025-05-10T21:11:19.447536Z",
     "shell.execute_reply": "2025-05-10T21:11:19.446971Z",
     "shell.execute_reply.started": "2025-05-10T21:11:17.941487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,441,472</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">11,845,152</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                           │                        │                │ encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ encoder (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m6,441,472\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
       "│ decoder (\u001b[38;5;33mFunctional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)    │     \u001b[38;5;34m11,845,152\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                           │                        │                │ encoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,286,624</span> (69.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,286,624\u001b[0m (69.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,286,624</span> (69.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,286,624\u001b[0m (69.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x_enc = keras_hub.layers.TokenAndPositionEmbedding(ENG_VOCAB_SIZE, MAX_SEQUENCE_LENGTH, EMBED_DIM)(encoder_inputs)\n",
    "encoder_outputs = keras_hub.layers.TransformerEncoder(INTERMEDIATE_DIM, NUM_HEADS)(inputs=x_enc)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs, name=\"encoder\")\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "x_dec = keras_hub.layers.TokenAndPositionEmbedding(TR_VOCAB_SIZE, MAX_SEQUENCE_LENGTH, EMBED_DIM)(decoder_inputs)\n",
    "decoder_mid = keras_hub.layers.TransformerDecoder(INTERMEDIATE_DIM, NUM_HEADS)(x_dec, encoded_seq_inputs)\n",
    "decoder_mid_dropout = keras.layers.Dropout(0.5)(decoder_mid)\n",
    "decoder_outputs_final = keras.layers.Dense(TR_VOCAB_SIZE, activation=\"softmax\")(decoder_mid_dropout)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs_final, name=\"decoder\")\n",
    "\n",
    "encoder_output_tensor = encoder(encoder_inputs)\n",
    "decoder_output_tensor = decoder([decoder_inputs, encoder_output_tensor])\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_output_tensor, name=\"transformer\")\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T21:11:19.448607Z",
     "iopub.status.busy": "2025-05-10T21:11:19.448376Z",
     "iopub.status.idle": "2025-05-11T00:12:54.339287Z",
     "shell.execute_reply": "2025-05-11T00:12:54.338585Z",
     "shell.execute_reply.started": "2025-05-10T21:11:19.448581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746911488.968717     118 service.cc:148] XLA service 0x7d31041b5770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1746911488.969891     118 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1746911488.969914     118 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "W0000 00:00:1746911489.362329     118 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "I0000 00:00:1746911490.226999     118 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   1/3896\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23:04:17\u001b[0m 21s/step - accuracy: 0.0000e+00 - loss: 9.8446"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746911500.640588     118 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3192/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1:16\u001b[0m 108ms/step - accuracy: 0.7821 - loss: 1.7830"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746911845.865232     118 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.7877 - loss: 1.7031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1746911932.923616     119 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1746911940.468368     118 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 117ms/step - accuracy: 0.7878 - loss: 1.7030 - val_accuracy: 0.8560 - val_loss: 0.8604\n",
      "Epoch 2/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 113ms/step - accuracy: 0.8555 - loss: 0.8798 - val_accuracy: 0.8750 - val_loss: 0.7188\n",
      "Epoch 3/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 113ms/step - accuracy: 0.8704 - loss: 0.7723 - val_accuracy: 0.8833 - val_loss: 0.6724\n",
      "Epoch 4/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 113ms/step - accuracy: 0.8779 - loss: 0.7262 - val_accuracy: 0.8870 - val_loss: 0.6525\n",
      "Epoch 5/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 112ms/step - accuracy: 0.8832 - loss: 0.6922 - val_accuracy: 0.8911 - val_loss: 0.6307\n",
      "Epoch 6/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 112ms/step - accuracy: 0.8876 - loss: 0.6644 - val_accuracy: 0.8932 - val_loss: 0.6185\n",
      "Epoch 7/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 112ms/step - accuracy: 0.8914 - loss: 0.6407 - val_accuracy: 0.8950 - val_loss: 0.6096\n",
      "Epoch 8/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 112ms/step - accuracy: 0.8945 - loss: 0.6215 - val_accuracy: 0.8970 - val_loss: 0.6014\n",
      "Epoch 9/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 111ms/step - accuracy: 0.8973 - loss: 0.6040 - val_accuracy: 0.8983 - val_loss: 0.5935\n",
      "Epoch 10/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 111ms/step - accuracy: 0.8998 - loss: 0.5877 - val_accuracy: 0.9001 - val_loss: 0.5869\n",
      "Epoch 11/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 112ms/step - accuracy: 0.9019 - loss: 0.5738 - val_accuracy: 0.9011 - val_loss: 0.5852\n",
      "Epoch 12/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 111ms/step - accuracy: 0.9039 - loss: 0.5618 - val_accuracy: 0.9016 - val_loss: 0.5834\n",
      "Epoch 13/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 111ms/step - accuracy: 0.9056 - loss: 0.5507 - val_accuracy: 0.9025 - val_loss: 0.5819\n",
      "Epoch 14/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 111ms/step - accuracy: 0.9072 - loss: 0.5410 - val_accuracy: 0.9025 - val_loss: 0.5807\n",
      "Epoch 15/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 111ms/step - accuracy: 0.9086 - loss: 0.5322 - val_accuracy: 0.9033 - val_loss: 0.5816\n",
      "Epoch 16/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 111ms/step - accuracy: 0.9101 - loss: 0.5230 - val_accuracy: 0.9041 - val_loss: 0.5813\n",
      "Epoch 17/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m433s\u001b[0m 111ms/step - accuracy: 0.9113 - loss: 0.5151 - val_accuracy: 0.9046 - val_loss: 0.5801\n",
      "Epoch 18/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 111ms/step - accuracy: 0.9127 - loss: 0.5076 - val_accuracy: 0.9047 - val_loss: 0.5828\n",
      "Epoch 19/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 111ms/step - accuracy: 0.9136 - loss: 0.5011 - val_accuracy: 0.9051 - val_loss: 0.5823\n",
      "Epoch 20/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 111ms/step - accuracy: 0.9146 - loss: 0.4942 - val_accuracy: 0.9053 - val_loss: 0.5844\n",
      "Epoch 21/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 111ms/step - accuracy: 0.9156 - loss: 0.4886 - val_accuracy: 0.9056 - val_loss: 0.5837\n",
      "Epoch 22/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 111ms/step - accuracy: 0.9167 - loss: 0.4820 - val_accuracy: 0.9058 - val_loss: 0.5873\n",
      "Epoch 23/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 111ms/step - accuracy: 0.9175 - loss: 0.4769 - val_accuracy: 0.9062 - val_loss: 0.5866\n",
      "Epoch 24/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m431s\u001b[0m 111ms/step - accuracy: 0.9184 - loss: 0.4717 - val_accuracy: 0.9065 - val_loss: 0.5890\n",
      "Epoch 25/25\n",
      "\u001b[1m3896/3896\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 110ms/step - accuracy: 0.9192 - loss: 0.4668 - val_accuracy: 0.9063 - val_loss: 0.5906\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "transformer.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(\"\\nStarting training...\")\n",
    "history = transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T00:12:54.340256Z",
     "iopub.status.busy": "2025-05-11T00:12:54.340057Z",
     "iopub.status.idle": "2025-05-11T00:12:54.346569Z",
     "shell.execute_reply": "2025-05-11T00:12:54.345831Z",
     "shell.execute_reply.started": "2025-05-11T00:12:54.340240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def decode_sequences(input_sentences_list):\n",
    "    batch_size = len(input_sentences_list)\n",
    "    encoder_tokens_ragged = eng_tokenizer(input_sentences_list)\n",
    "    eng_packer = keras_hub.layers.StartEndPacker(sequence_length=MAX_SEQUENCE_LENGTH, pad_value=eng_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    encoder_input_data = eng_packer(encoder_tokens_ragged)\n",
    "    start_id = tr_tokenizer.token_to_id(\"[START]\")\n",
    "    pad_id_tr = tr_tokenizer.token_to_id(\"[PAD]\")\n",
    "    prompt = tf.concat([tf.fill([batch_size, 1], start_id), tf.fill([batch_size, MAX_SEQUENCE_LENGTH - 1], pad_id_tr)], axis=1)\n",
    "    prompt = ops.cast(prompt, dtype=\"int64\")\n",
    "\n",
    "    def next_fn(current_decoder_input, cache, index):\n",
    "        logits = transformer([encoder_input_data, current_decoder_input])[:, index - 1, :]\n",
    "        return logits, None, cache\n",
    "\n",
    "    greedy_sampler = keras_hub.samplers.GreedySampler()\n",
    "    generated_tokens = greedy_sampler(\n",
    "        next_fn,\n",
    "        prompt,\n",
    "        stop_token_ids=[tr_tokenizer.token_to_id(\"[END]\")],\n",
    "        index=1,\n",
    "    )\n",
    "    generated_sentences = tr_tokenizer.detokenize(generated_tokens)\n",
    "    return [s.numpy().decode(\"utf-8\") if isinstance(s, tf.Tensor) else s for s in generated_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T00:12:54.347538Z",
     "iopub.status.busy": "2025-05-11T00:12:54.347311Z",
     "iopub.status.idle": "2025-05-11T00:13:02.094130Z",
     "shell.execute_reply": "2025-05-11T00:13:02.093486Z",
     "shell.execute_reply.started": "2025-05-11T00:12:54.347516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Translation Examples ---\n",
      "** Example 1 **\n",
      "SRC: latin word order is free, but not arbitrary.\n",
      "TRG: latincederek ücretsizdir ama aksine , ama aksine .\n",
      "\n",
      "** Example 2 **\n",
      "SRC: i might try to do that.\n",
      "TRG: bunu yapmaya çalışabilirim .\n",
      "\n",
      "** Example 3 **\n",
      "SRC: i know tom is very amusing.\n",
      "TRG: tom ' un çok komik olduğunu biliyorum .\n",
      "\n",
      "** Example 4 **\n",
      "SRC: our music teacher advised me to visit vienna.\n",
      "TRG: müzik öğretmenimiz beni ziyaret etmemi tavsiye etti .\n",
      "\n",
      "** Example 5 **\n",
      "SRC: none of my friends speak french.\n",
      "TRG: arkadaşlarımdan hiçbiri fransızca konuşmaz .\n",
      "\n",
      "** Example 6 **\n",
      "SRC: how many teeth does a horse have?\n",
      "TRG: kaç tane diş fer var ?\n",
      "\n",
      "** Example 7 **\n",
      "SRC: thank you again for your kind assistance.\n",
      "TRG: senin nazik yardımını tekrar teşekkürler .\n",
      "\n",
      "** Example 8 **\n",
      "SRC: he's on his last legs.\n",
      "TRG: o , son ayağında , .\n",
      "\n",
      "** Example 9 **\n",
      "SRC: i'm anxious to know the results of the blood test.\n",
      "TRG: kan testin sonuçları bilmekten endişeliyim .\n",
      "\n",
      "** Example 10 **\n",
      "SRC: tom was angry, but mary wasn't.\n",
      "TRG: tom kızgındı , ama mary değildi .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Translation Examples ---\")\n",
    "if test_pairs:\n",
    "    test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "    if test_eng_texts:\n",
    "        for i in range(min(10, len(test_eng_texts))):\n",
    "            input_sentence = random.choice(test_eng_texts)\n",
    "            translations = decode_sequences([input_sentence])\n",
    "            translated = translations[0].replace(\"[PAD]\", \"\").replace(\"[START]\", \"\").replace(\"[END]\", \"\").strip()\n",
    "            print(f\"** Example {i+1} **\\nSRC: {input_sentence}\\nTRG: {translated}\\n\")\n",
    "    else:\n",
    "        print(\"Test için İngilizce cümle bulunamadı.\")\n",
    "else:\n",
    "    print(\"`test_pairs` listesi bulunamadı veya boş. Çeviri örnekleri atlanıyor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of Qualitative Assessment (Task d):\n",
    "\n",
    "- The model performs quite well on simple and direct sentences (Examples 2, 3, 5, 10).\n",
    "- There are minor structural or word-choice errors in some sentences, but the main idea is generally understandable (Examples 4, 7, 9).\n",
    "- The model struggles with idioms (Example 8) or more complex structures/wordplay (Examples 1, 6), tending towards word-for-word translations. The mistranslation of \"free\" as \"ücretsizdir\" (free of charge) instead of \"serbest\" (free/unconstrained) (Example 1), or the generation of a nonsensical word like \"fer\" for \"horse\" (Example 6), indicates situations where the model may not fully grasp the context or has encountered insufficient data for those specific cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T00:13:02.095007Z",
     "iopub.status.busy": "2025-05-11T00:13:02.094791Z",
     "iopub.status.idle": "2025-05-11T00:15:20.666751Z",
     "shell.execute_reply": "2025-05-11T00:15:20.665991Z",
     "shell.execute_reply.started": "2025-05-11T00:13:02.094988Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation on Test Set ---\n",
      "Evaluating on 300 test samples...\n",
      "  Evaluated 50/300 samples...\n",
      "  Evaluated 100/300 samples...\n",
      "  Evaluated 150/300 samples...\n",
      "  Evaluated 200/300 samples...\n",
      "  Evaluated 250/300 samples...\n",
      "Evaluation finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Evaluation on Test Set ---\")\n",
    "rouge_1_eval = keras_hub.metrics.RougeN(order=1, name='rouge_1_eval')\n",
    "rouge_2_eval = keras_hub.metrics.RougeN(order=2, name='rouge_2_eval')\n",
    "rouge_l_eval = keras_hub.metrics.RougeL(name='rouge_l_eval')\n",
    "\n",
    "if test_pairs:\n",
    "    num_eval_samples = min(len(test_pairs), 300)\n",
    "    print(f\"Evaluating on {num_eval_samples} test samples...\")\n",
    "    for i, (input_sentence, reference_sentence) in enumerate(test_pairs[:num_eval_samples]):\n",
    "        if i % 50 == 0 and i > 0: print(f\"  Evaluated {i}/{num_eval_samples} samples...\")\n",
    "        translations = decode_sequences([input_sentence])\n",
    "        translated = translations[0].replace(\"[PAD]\", \"\").replace(\"[START]\", \"\").replace(\"[END]\", \"\").strip()\n",
    "        rouge_1_eval.update_state(reference_sentence, translated)\n",
    "        rouge_2_eval.update_state(reference_sentence, translated)\n",
    "        rouge_l_eval.update_state(reference_sentence, translated)\n",
    "    print(\"Evaluation finished.\")\n",
    "\n",
    "    r1_result = rouge_1_eval.result()\n",
    "    r2_result = rouge_2_eval.result()\n",
    "    rl_result = rouge_l_eval.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T00:15:20.676425Z",
     "iopub.status.busy": "2025-05-11T00:15:20.676237Z",
     "iopub.status.idle": "2025-05-11T00:15:20.694726Z",
     "shell.execute_reply": "2025-05-11T00:15:20.694213Z",
     "shell.execute_reply.started": "2025-05-11T00:15:20.676408Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ROUGE Scores ---\n",
      "ROUGE-1 Scores:\n",
      "  Precision: 0.6922\n",
      "  Recall:    0.6935\n",
      "  F1 Score:  0.6764\n",
      "ROUGE-2 Scores:\n",
      "  Precision: 0.5176\n",
      "  Recall:    0.5178\n",
      "  F1 Score:  0.5128\n",
      "ROUGE-L Scores:\n",
      "  Precision: 0.6803\n",
      "  Recall:    0.6815\n",
      "  F1 Score:  0.6746\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- ROUGE Scores ---\")\n",
    "print(f\"ROUGE-1 Scores:\\n  Precision: {r1_result['precision'].numpy():.4f}\\n  Recall:    {r1_result['recall'].numpy():.4f}\\n  F1 Score:  {r1_result['f1_score'].numpy():.4f}\")\n",
    "print(f\"ROUGE-2 Scores:\\n  Precision: {r2_result['precision'].numpy():.4f}\\n  Recall:    {r2_result['recall'].numpy():.4f}\\n  F1 Score:  {r2_result['f1_score'].numpy():.4f}\")\n",
    "print(f\"ROUGE-L Scores:\\n  Precision: {rl_result['precision'].numpy():.4f}\\n  Recall:    {rl_result['recall'].numpy():.4f}\\n  F1 Score:  {rl_result['f1_score'].numpy():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7385211,
     "sourceId": 11763784,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7375079,
     "sourceId": 11748057,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
